{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":["opMjKTY-O_X1","XJ9dIkn3PRI7"],"provenance":[],"gpuType":"V5E1"},"accelerator":"TPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14738953,"sourceType":"datasetVersion","datasetId":9381448}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"youtube_url = \"https://youtu.be/7jVP22C0jxY?si=YoKlbc65PZCUgN0v\"\n\nadditional_prompt = \"\"\"\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from urllib.parse import urlparse\n\ndef normalize_youtube_url(youtube_url: str) -> str:\n    parsed = urlparse(youtube_url)\n\n    # Handle youtu.be short links\n    if parsed.netloc == \"youtu.be\":\n        video_id = parsed.path.lstrip(\"/\")\n        return f\"https://www.youtube.com/watch?v={video_id}\"\n\n    # Already a full YouTube URL (or something else)\n    return youtube_url\n\n\nyoutube_url = normalize_youtube_url(youtube_url)\nprint(youtube_url)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Installation","metadata":{"id":"opMjKTY-O_X1"}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%capture\n# !pip cache purge\n# !pip uninstall -y numpy scikit-learn\n# !pip install scikit-learn \n# !pip uninstall -y numpy \n# !pip install \"numpy<2.0\" ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install yt-dlp whisperx openai-whisper\n!pip install -U yt-dlp-ejs","metadata":{"id":"E-frTKxyIuVZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install ultralytics moviepy opencv-python ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%capture\n# !pip install --upgrade scikit-learn scipy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Notifier","metadata":{}},{"cell_type":"code","source":"import requests\nimport os\n\nclass FileUploader:\n    def __init__(self, bot_token, chat_id):\n        self.bot_token = bot_token\n        self.chat_id = chat_id\n        self.tg_base_url = f\"https://api.telegram.org/bot{bot_token}\"\n\n    def send_to_telegram(self, file_path):\n        \"\"\"Attempts to upload a file directly to Telegram.\"\"\"\n        url = f\"{self.tg_base_url}/sendDocument\"\n        with open(file_path, 'rb') as f:\n            response = requests.post(\n                url, \n                data={'chat_id': self.chat_id}, \n                files={'document': f}, \n                timeout=60\n            )\n        response.raise_for_status()  # Raises exception if status is not 200\n        return True\n\n    def upload_to_gofile(self, file_path):\n        \"\"\"Uploads a file to GoFile and returns the download link.\"\"\"\n        url = \"https://upload.gofile.io/uploadfile\"\n        with open(file_path, 'rb') as f:\n            response = requests.post(url, files={'file': f}, timeout=120)\n        \n        response.raise_for_status()\n        data = response.json()\n        \n        if data.get(\"status\") == \"ok\":\n            return data[\"data\"][\"downloadPage\"]\n        raise Exception(f\"GoFile error: {data.get('status')}\")\n\n    def send_text_message(self, text, parse_mode = \"Markdown\"):\n        \"\"\"Helper to send a text message/link to Telegram.\"\"\"\n        url = f\"{self.tg_base_url}/sendMessage\"\n        payload = {\n            'chat_id': self.chat_id,\n            'text': text,\n            'parse_mode': parse_mode\n        }\n        return requests.post(url, data=payload)\n\n    def process_files(self, file_paths):\n        \"\"\"Main loop: process each file and handle fallback.\"\"\"\n        for path in file_paths:\n            file_name = os.path.basename(path)\n            \n            if not os.path.exists(path):\n                print(f\"[-] Skipped: {file_name} (File not found)\")\n                continue\n\n            try:\n                print(f\"[*] Uploading to Telegram: {file_name}\")\n                self.send_to_telegram(path)\n                print(f\"[+] Success: {file_name} sent to Telegram.\")\n\n            except Exception as e:\n                print(f\"[!] Telegram failed for {file_name}. Switching to GoFile...\")\n                self._handle_gofile_fallback(path, file_name)\n\n    def _handle_gofile_fallback(self, path, file_name):\n        \"\"\"Internal helper to manage the fallback process.\"\"\"\n        try:\n            link = self.upload_to_gofile(path)\n            msg = f\"ðŸ“‚ *File:* `{file_name}`\\nâš ï¸ Size limit exceeded.\\nðŸ”— [Download from GoFile]({link})\"\n            self.send_text_message(msg)\n            print(f\"[+] Success: {file_name} link sent via GoFile.\")\n        except Exception as e:\n            print(f\"[-] Critical: Failed to upload {file_name} to both services. Error: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nCHAT_ID = user_secrets.get_secret(\"TELEGRAM_CHAT_ID\")\nTOKEN = user_secrets.get_secret(\"TELEGRAM_TOKEN\")\n\n# Initialize and Run\nuploader = FileUploader(TOKEN, CHAT_ID)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download Source","metadata":{"id":"1PyFKKTlPDNT"}},{"cell_type":"code","source":"import yt_dlp\nimport os\nimport datetime\nimport re\n\nurl = youtube_url\n\n# Define base path in Google Drive\nbase_drive_path = \"/kaggle/working/\"\n\n# Options for extracting info (without downloading)\ninfo_opts = {\n    'quiet': True,\n    'skip_download': True,\n    'forcetitle': True,\n}\n\n# Use yt_dlp to get video information\ntry:\n    with yt_dlp.YoutubeDL(info_opts) as ydl:\n        info_dict = ydl.extract_info(url, download=False)\n        video_title = info_dict.get('title', 'Unknown_Video')\n        # Sanitize title for filename: replace non-alphanumeric (except space, dash, underscore, dot) with underscore\n        video_title = re.sub(r'[\\\\/*?:\"<>;|=]', '_', video_title)\n        video_title = re.sub(r'\\s+', ' ', video_title).strip() # Remove multiple spaces and leading/trailing spaces\n        video_title = re.sub(r'[^\\w\\s\\-.]', '', video_title) # Remove other special characters except alphanumeric, space, dash, dot\n        video_title = video_title.replace(' ', '_') # Replace spaces with underscores for directory name\n\n\n    # Create dynamic directory name\n    date_str = datetime.date.today().strftime('%Y-%m-%d')\n    global output_base_path\n    output_base_path = os.path.join(base_drive_path, f\"{date_str}-{video_title}\")\n\n    # Ensure output directory exists\n    os.makedirs(output_base_path, exist_ok=True)\n    print(f\"Output directory set to: {output_base_path}\")\n\n    # Options for the actual download\n    ydl_opts = {\n        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best', # Prioritize mp4, then best available\n        'outtmpl': os.path.join(output_base_path, f\"{video_title}.%(ext)s\"), # Save file with dynamic name in dynamic folder\n        'writedescription': False, # Don't write extra files\n        'writeinfojson': False,\n        'writesubtitles': False,\n        'fixup': 'detect_or_warn', # Fix potential issues with containers\n        'restrictfilenames': True, # Keep filenames safe\n        'trim_filenames': 200, # Truncate long filenames\n    }\n\n    # download 1080 at max\n    ydl_opts = {\n        # Limit height to 1080p or lower, while merging best audio\n        'format': 'bestvideo[height<=1080]+bestaudio/best[height<=1080]/best',\n        \n        # Ensure the final output is merged into an mp4 container\n        'merge_output_format': 'mp4',\n        \n        # Your existing settings\n        'outtmpl': os.path.join(output_base_path, f\"{video_title}.%(ext)s\"),\n        'writedescription': False,\n        'writeinfojson': False,\n        'writesubtitles': False,\n        'fixup': 'detect_or_warn',\n        'restrictfilenames': True,\n        'trim_filenames': 200,\n    }\n\n    print(f\"Downloading video '{video_title}' to {output_base_path}...\")\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([url])\n    print(\"Download completed!\")\n\n    # Define global file paths\n    global video_file_path, audio_file_path, transcription_json_path, clips_json_path\n    video_file_path = os.path.join(output_base_path, f\"{video_title}.mp4\") # Assuming mp4 for video\n    audio_file_path = os.path.join(output_base_path, f\"{video_title}.mp3\") # Assuming mp3 for audio\n    transcription_json_path = os.path.join(output_base_path, \"transcribe.json\")\n    clips_json_path = os.path.join(output_base_path, \"clips.json\")\n\n    print(f\"Video will be at: {video_file_path}\")\n    print(f\"Audio will be at: {audio_file_path}\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    uploader.send_text_message(f'Error: {e} \\n\\nYouTube: {youtube_url}')","metadata":{"id":"CmrH7t1OKwZ5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Core Logic","metadata":{"id":"cnZYncXMPNOg"}},{"cell_type":"markdown","source":"### Transcribe","metadata":{"id":"IXocY2DYRv9t"}},{"cell_type":"code","source":"import subprocess\n\n# Use global variables for paths\ninput_video = video_file_path\noutput_audio = audio_file_path\n\ncmd = [\n    \"ffmpeg\",\n    \"-i\", input_video,\n    \"-vn\", # No video\n    \"-acodec\", \"libmp3lame\", # Audio codec for mp3\n    \"-q:a\", \"2\", # Quality setting for mp3 (2 is good quality)\n    output_audio\n]\n\n# cmd = [\n#     \"ffmpeg\",\n#     \"-i\", input_video,\n#     \"-vn\",\n#     \"-c:a\", \"copy\", # <--- COPY instead of re-encode\n#     output_audio # Make sure this path ends in .m4a or .aac, not .mp3 (unless source is mp3)\n# ]\n\ntry:\n    print(f\"Extracting audio from {input_video} to {output_audio}...\")\n    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n    print(\"Audio extraction completed successfully!\")\n    print(result.stdout)\nexcept FileNotFoundError:\n    print(\"Error: 'ffmpeg' command not found. Please ensure FFmpeg is installed and in your system PATH.\")\nexcept subprocess.CalledProcessError as e:\n    print(f\"Error during audio extraction: {e}\")\n    print(f\"FFmpeg stdout:\\n{e.stdout}\")\n    print(f\"FFmpeg stderr:\\n{e.stderr}\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")","metadata":{"id":"265a9238","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Set a new environment variable\nos.environ['TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD'] = \"true\"","metadata":{"id":"L7ZyLtyBw-bb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import whisperx\nimport gc\nimport torch\nimport json\nimport os\nimport sys\nimport fileinput\nimport omegaconf # Import omegaconf for the fix\n\n# --- Start Patch for whisperx/asr.py ---\nwhisperx_asr_path = None\nfor p in sys.path:\n    candidate_path = os.path.join(p, 'whisperx', 'asr.py')\n    if os.path.exists(candidate_path):\n        whisperx_asr_path = candidate_path\n        break\n\nif whisperx_asr_path:\n    print(f\"Patching {whisperx_asr_path}...\")\n    original_line = \"from transformers import Pipeline\"\n    new_line = \"from transformers.pipelines import Pipeline\"\n    patched = False\n\n    # Use fileinput for in-place editing safely\n    with fileinput.FileInput(whisperx_asr_path, inplace=True) as file:\n        for line in file:\n            if original_line in line:\n                sys.stdout.write(line.replace(original_line, new_line))\n                patched = True\n            else:\n                sys.stdout.write(line)\n\n    if patched:\n        print(\"Patch applied successfully.\")\n    else:\n        print(\"Original import line not found in asr.py, patch may not be needed or already applied.\")\nelse:\n    print(\"Could not find whisperx/asr.py to patch. Transcription might fail.\")\n# --- End Patch ---\n\n# Add omegaconf.listconfig.ListConfig to safe globals for torch.load\ntorch.serialization.add_safe_globals([omegaconf.listconfig.ListConfig])\n\ndef transcribe_with_whisperx(audio_path, batch_size=16, language=None):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    compute_type = \"float16\" if torch.cuda.is_available() else \"int8\" # float16 for GPU, int8 for CPU\n    print(f\"Using device: {device} with compute type: {compute_type}\")\n\n    # 1. Load audio\n    audio = whisperx.load_audio(audio_path)\n\n    # 2. Load model\n    model_name = \"large-v3-turbo\" #large-v3-turbo #large-v2\n    model = whisperx.load_model(model_name, device=device, compute_type=compute_type, language=language)\n\n    # 3. Transcribe audio\n    print(\"Transcribing audio...\")\n    result = model.transcribe(audio, batch_size=batch_size)\n    # print(result[\"segments\"])\n\n    # delete model from GPU memory\n    del model\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    # 4. Align whisper output\n    print(\"Aligning transcription...\")\n    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n    aligned_result = whisperx.align(result[\"segments\"], model_a, audio, device, return_char_alignments=False)\n    # print(aligned_result[\"segments\"])\n\n    # delete model from GPU memory\n    del model_a\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return aligned_result\n\ndef run_whisperx_transcription(audio_file_path, language='id', output_json_path='whisperx_transcription.json'):\n    print(f\"Running WhisperX transcription for {audio_file_path}...\")\n    # This already returns the aligned segments as dict\n    whisperx_transcription_result = transcribe_with_whisperx(audio_file_path, language=language)\n\n    # Print the raw whisperx output for verification\n    print(json.dumps(whisperx_transcription_result, indent=2, ensure_ascii=False))\n\n    # Save the raw whisperx output to a file\n    with open(output_json_path, 'w', encoding='utf-8') as f:\n        json.dump(whisperx_transcription_result, f, indent=2, ensure_ascii=False)\n    print(f\"\\nRaw WhisperX transcription saved to {output_json_path}\")\n    return whisperx_transcription_result\n","metadata":{"id":"4feae562","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import whisper\nimport json\n\ndef run_openai_whisper_transcription(audio_path, language=\"id\", model_size=\"small\", output_json_path='openai_whisper_transcription.json'):\n    print(f\"Running OpenAI Whisper ({model_size}) transcription for {audio_path}...\")\n    # 1. Load the Whisper model\n    # 'small' is a good balance of speed and accuracy; use 'medium' or 'large' for better accuracy\n    model = whisper.load_model(model_size)\n\n    # 2. Transcribe the audio\n    # language='id' forces Bahasa Indonesia\n    # word_timestamps=True allows us to extract start/end times for every word\n    result = model.transcribe(audio_path, language=language, word_timestamps=True)\n\n    # 3. Print or Save the JSON output\n    json_output = json.dumps(result, indent=2, ensure_ascii=False)\n    # print(json_output)\n\n    # Optional: Save to a file\n    with open(output_json_path, \"w\", encoding='utf-8') as f:\n        f.write(json_output)\n    print(f\"\\nRaw Transcription saved to {output_json_path}\")\n    return result\n","metadata":{"id":"Sx9zXYJrzLyl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcribe = run_openai_whisper_transcription(audio_file_path, \"id\", output_json_path=transcription_json_path, model_size='medium')","metadata":{"id":"JM6NnqqMzRNW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Analyze","metadata":{"id":"QLQg49MkRytF"}},{"cell_type":"code","source":"import json\nimport math\n\ndef format_transcription_for_ai(transcription_json_path):\n    \"\"\"\n    Formats the raw transcription JSON into a simplified string for AI context\n    and creates a map for original segment details.\n\n    Args:\n        transcription_json_path (str): Path to the raw transcription JSON file (e.g., from OpenAI Whisper).\n\n    Returns:\n        tuple: A tuple containing:\n            - str: Formatted transcription string for AI context.\n            - dict: A dictionary mapping segment IDs to their original start, end, and text.\n    \"\"\"\n    try:\n        with open(transcription_json_path, 'r', encoding='utf-8') as f:\n            raw_transcription = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: Transcription file not found at {transcription_json_path}\")\n        return \"\", {}\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {transcription_json_path}\")\n        return \"\", {}\n\n    formatted_strings = []\n    segment_map = {}\n\n    # The raw_transcription from OpenAI Whisper has a 'segments' key\n    segments = raw_transcription.get('segments', [])\n\n    for segment in segments:\n        segment_id = segment.get('id')\n        start_time = segment.get('start')\n        end_time = segment.get('end')\n        text = segment.get('text', '').strip()\n\n        if segment_id is not None and start_time is not None and end_time is not None:\n            # For the AI context string, simplify times to integers for readability\n            # Example: [0] (0-5s) Text\n            simplified_start = math.floor(start_time)\n            simplified_end = math.ceil(end_time)\n            formatted_str = f\"[{segment_id}] ({simplified_start}-{simplified_end}s) {text}\"\n            formatted_strings.append(formatted_str)\n\n            # Store original (float) times in the map\n            segment_map[segment_id] = {\n                'start': start_time,\n                'end': end_time,\n                'text': text\n            }\n        else:\n            print(f\"Warning: Skipping malformed segment: {segment}\")\n\n    ai_context_string = \"\\n\".join(formatted_strings)\n\n    return ai_context_string, segment_map\n","metadata":{"id":"S-xnFAoRTY4a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transcription_json_path='/content/transcribe.json'\n# clips_json_path='clips.json'","metadata":{"id":"1r2AXsRdVdTV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of how to use this function (this will not run automatically)\nai_context, clip_map = format_transcription_for_ai(transcription_json_path)\nprint(\"\\n--- AI Context String ---\")\nprint(ai_context)\nprint(\"\\n--- Segment Map ---\")\nprint(clip_map)","metadata":{"id":"BUD5JZuSWQkH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extract Video Segments With AI","metadata":{}},{"cell_type":"code","source":"instruction_1 = \"\"\"\nYou are a Senior Viral Content Strategist and Elite Video Editor for TikTok and YouTube Shorts.\nYour goal is not just to \"find parts,\" but to **architect viral moments** that retain 100% viewer attention.\n\n### INPUT DATA:\nThe transcription segments are given in the format: `[segment_ID] (start_seconds-end_seconds) Text`.\n\n### CRITICAL SELECTION CRITERIA (The \"Viral Formula\"):\n1.  **The \"0:03 Hook\" Rule:** The FIRST segment_id you select must be a \"Golden Sentence.\" It must be a controversial statement, a high-emotion reaction, or a high-value promise. DO NOT start with \"Hello,\" \"Welcome,\" or context. Start with the Action.\n2.  **Dead Air Removal:** ruthlessly skip segments that contain filler (ums, ahs, long pauses, irrelevant pleasantries).\n3.  **Reordering for Impact:** You are allowed to take a punchline from the middle/end of a thought and place it at the very beginning to serve as the Hook, then cut back to the explanation.\n4.  **Duration:** Total clip length must be between 40 and 120 seconds (sweet spot for retention), up to 240s only if the story is incredibly gripping.\n\n### EXECUTION PLAN REQUIREMENTS:\nYou must provide a director's plan for *how* to edit this specific clip to ensure virality.\n\n### OUTPUT FORMAT:\nYour output MUST be a valid JSON array. Each element is an object with the following structure:\n{\n  \"title\": \"A clickbait-style title (under 50 chars) using ALL CAPS for emphasis words\",\n  \"viral_score\": \"Integer 1-10 based on emotional impact\",\n  \"hook_segment_id\": \"The ID of the single best sentence to start the video with\",\n  \"segments_ids\": [Array of integers NOTE: The order here represents the EDITING order, not necessarily the chronological order],\n  \"reasoning\": \"Why this specific part will go viral (e.g., 'Polarizing opinion', 'Universal relatable struggle')\",\n  \"execution_plan\": {\n    \"visual_hook\": \"Description of the screen text/visual to use in the first 3 seconds (e.g., 'Stop doing this!')\",\n    \"b_roll_suggestions\": \"What images/videos should overlay the speaker? (e.g., 'Show stock footage of private jets')\",\n    \"music_mood\": \"The specific genre/vibe of background music (e.g., 'Interstellar dramatic', 'Lofi hiphop')\",\n    \"editing_notes\": \"Specific instructions (e.g., 'Zoom in on face at timestamp X', 'Add vine boom sound effect at ID Y')\"\n  }\n}\n\n### CRITICAL RULE:\nDo not give generic advice like \"Add captions.\" Be specific. Imagine you are talking to a junior editor who needs to know exactly **what image** to put and **what sound** to play.\nDO NOT CUT AT THE MIDDLE OF SENTENCE, prefer to add more sentence even will make video a bit longer.\n\nEnsure the generated JSON is valid and parseable. Do not include markdown formatting outside the JSON block.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"instruction_2 = \"\"\"\nYou are a Senior Narrative Designer for Short-Form Video.\nYour goal is to extract a cohesive, viral short video from the provided transcript segments based on the \"Infinite Loop\" storytelling structure.\n\n### INPUT DATA:\nSegments are provided as: `[segment_ID] (timestamp) Text`.\nNOTE: These segments are generated by AI (Whisper) and often split in the middle of sentences.\n\n### PRIORITY DIRECTIVE 1: SENTENCE INTEGRITY PROTOCOL\nThe AI transcription frequently breaks sentences into two segments.\n- NEVER select a segment that starts mid-thought (e.g., starting with \"and,\" \"but,\" \"which is why\") unless you also select the previous segment.\n- NEVER select a segment that ends mid-thought unless you also select the next segment.\n- **Rule:** If you want a specific line, you must grab the \"surrounding neighbors\" to ensure the sentence is grammatically complete. It is better to have a clip that is 2 seconds too long than a clip that cuts off a word.\n\n### PRIORITY DIRECTIVE 2: THE \"INFINITE LOOP\" STRUCTURE\nYou must arrange the selected segments to fit this specific narrative arc:\n\n1.  **THE HOOK (0-5s):** A curiosity gap, a bold claim, or a \"This is how I...\" statement. It must stop the scroll.\n2.  **THE CONTEXT (5-15s):** Quickly explain the \"Why\" or the background info needed to understand the hook.\n3.  **THE STORY/MEAT (15-50s):** The core explanation, the struggle, or the steps. This is the value delivery.\n4.  **THE CONCLUSION (Last 5-10s):** The payoff or result. Ideally, the last sentence should seamlessly flow back into the Hook (e.g., ending with \"...and that is why...\").\n\n### EXCLUSION CRITERIA:\n- Remove \"Hello guys,\" \"Welcome back,\" and self-promotions.\n- Remove filler words (\"um,\" \"ah\") if they are their own isolated segments.\n\n### OUTPUT FORMAT:\nReturn a valid JSON object.\n{\n  \"title\": \"A 3-word punchy concept title\",\n  \"narrative_arc\": {\n    \"hook_ids\": [Array of IDs for the Hook],\n    \"context_ids\": [Array of IDs for the Context],\n    \"story_ids\": [Array of IDs for the Story],\n    \"conclusion_ids\": [Array of IDs for the Conclusion]\n  },\n  \"final_clip_sequence\": [Array of ALL integers combined in editing order],\n  \"reasoning\": \"Brief explanation of why this specific flow creates a curiosity gap.\"\n}\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\nimport os\n# from google.colab import userdata\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\ngemini_api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\n\n# gemini_api_key = userdata.get('GEMINI_API_KEY')\n\ndef generate_text_with_genai(prompt):\n    \"\"\"\n    Generates text using Google Generative AI.\n\n    Args:\n        prompt (str): The input prompt for the LLM.\n\n    Returns:\n        str: The generated text from the LLM.\n    \"\"\"\n    \n\n    client = genai.Client(\n        api_key=gemini_api_key,\n    )\n\n    model = \"gemini-3-flash-preview\"\n    contents = [\n        types.Content(\n            role=\"user\",\n            parts=[\n                types.Part.from_text(text=prompt),\n            ],\n        ),\n    ]\n    generate_content_config = types.GenerateContentConfig(\n        thinking_config=types.ThinkingConfig(\n            thinking_budget=1,\n        ),\n        system_instruction=[\n            types.Part.from_text(text=instruction_1),\n        ],\n    )\n\n    print(f\"Sending prompt to LLM: {prompt[:100]}...\")\n    try:\n        response = client.models.generate_content(\n            model=model,\n            contents=contents,\n            config=generate_content_config)\n        generated_text = response.text\n        print(\"Text generation completed.\")\n        return generated_text\n    except Exception as e:\n        print(f\"Error generating text with GenAI: {e}\")\n        return f\"Error: {e}\"\n","metadata":{"id":"7kUNI5VDXR_p","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ai_prompt = f\"\"\"\n## Additional Instruction\n{additional_prompt}\n\n## Context\n{ai_context}\n\"\"\"\n\nai_clips = generate_text_with_genai(ai_prompt)","metadata":{"id":"48r9Ha1lagXG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# replace markdown ```json\nai_clips = ai_clips.replace('```json', '').replace('```', '')\n# print(ai_clips)","metadata":{"id":"PkRM4cdtguAe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"raw_ai_clips.txt\"\n\ntry:\n    # Open the file in write mode ('w') and assign it to a file object 'f'\n    with open(file_path, 'w', encoding='utf-8') as f:\n        f.write(ai_clips) # Use the write() method to write the string\n    print(f\"Successfully wrote the string to {file_path}\")\nexcept IOError as e:\n    print(f\"An error occurred: {e}\")\n","metadata":{"id":"phJiBbHqX3g3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Clippig Video","metadata":{"id":"XJ9dIkn3PRI7"}},{"cell_type":"code","source":"import json\n\n# 2. Parse the ai_clips string into a Python list of dictionaries\ntry:\n    ai_clips_parsed = json.loads(ai_clips)\n    if not isinstance(ai_clips_parsed, list):\n        print(\"Warning: AI output is not a list. Assuming a single clip object.\")\n        ai_clips_parsed = [ai_clips_parsed] # Wrap in list if it's a single object\nexcept json.JSONDecodeError as e:\n    print(f\"Error decoding AI clips JSON: {e}\")\n    print(f\"Raw AI output: {ai_clips}\")\n    ai_clips_parsed = [] # Initialize as empty list to prevent further errors\nexcept Exception as e:\n    print(f\"An unexpected error occurred while parsing AI clips: {e}\")\n    ai_clips_parsed = []\n\nediting_instructions = \"\"\nfor i, clip in enumerate(ai_clips_parsed):\n    plan = clip.get('execution_plan', {})\n    editing_instructions += f\"\"\"\n    *__{i+1}. {clip.get('title')}__*\n    _VISUAL HOOK_\n    {plan.get(\"visual_hook\")}\n\n    _B-ROLL SUGGESTION_\n    {plan.get('b_roll_suggestions')}\n\n    _MUSIC_\n    {plan.get('music_mood')}\n\n    _NOTES_\n    {plan.get('editing_notes')}\n    \"\"\"\n\nprint(editing_instructions)\n\n# 3. Initialize an empty list called formatted_clips\nformatted_clips = []\n\n# 4. Iterate through each clip_suggestion in the parsed ai_clips list\nfor clip_suggestion in ai_clips_parsed:\n    # 5. For each clip_suggestion, extract its title, description, and segments_ids\n    title = clip_suggestion.get('title', 'Untitled Clip')\n    description = clip_suggestion.get('description', 'No description provided.')\n    segments_ids = clip_suggestion.get('segments_ids', [])\n\n    if not segments_ids:\n        print(f\"Warning: Clip '{title}' has no segment IDs. Skipping. -> {clip_suggestion}\")\n        continue\n\n    # 6. Initialize an empty list called current_clip_segments\n    current_clip_segments = []\n\n    # 7. Iterate through each segment_id in clip_suggestion['segments_ids']\n    for segment_id in segments_ids:\n        # 8. Look up the start and end times for the current segment_id from the clip_map dictionary\n        segment_details = clip_map.get((segment_id)) # clip_map keys are strings\n        if segment_details:\n            start_time = segment_details.get('start')\n            end_time = segment_details.get('end')\n            # 9. Append a dictionary {'start': start_time, 'end': end_time} to current_clip_segments\n            current_clip_segments.append({'start': start_time, 'end': end_time})\n        else:\n            print(f\"Warning: Segment ID {segment_id} not found in clip_map for clip '{title}'. Skipping.\")\n\n    if current_clip_segments:\n        # 10. Create a new dictionary\n        formatted_clip = {\n            'title': title,\n            'description': description,\n            'segments': current_clip_segments\n        }\n        # 11. Append this new dictionary to the formatted_clips list\n        formatted_clips.append(formatted_clip)\n\n# 12. Save the formatted_clips list to a JSON file named 'clips.json'\noutput_clips_json_path = clips_json_path\nwith open(output_clips_json_path, 'w', encoding='utf-8') as f:\n    json.dump(formatted_clips, f, indent=2, ensure_ascii=False)\n\nprint(f\"Formatted clips saved to {output_clips_json_path}\")\nprint(\"Content of formatted_clips:\")\nprint(json.dumps(formatted_clips, indent=2, ensure_ascii=False))","metadata":{"id":"45ad0a0f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# CONFIGURATION\nPADDING_DURATION = 2.0  # Seconds to add if gap is large enough\n\n# 1. Parse the ai_clips string\ntry:\n    ai_clips_parsed = json.loads(ai_clips)\n    if not isinstance(ai_clips_parsed, list):\n        ai_clips_parsed = [ai_clips_parsed]\nexcept Exception as e:\n    print(f\"Error parsing AI clips: {e}\")\n    ai_clips_parsed = []\n\nformatted_clips = []\n\nfor clip_suggestion in ai_clips_parsed:\n    title = clip_suggestion.get('title', 'Untitled Clip')\n    description = clip_suggestion.get('description', '')\n    raw_segment_ids = clip_suggestion.get('segments_ids', [])\n\n    if not raw_segment_ids:\n        continue\n\n    # --- PREPARATION STEP ---\n    # 1. Validate IDs exist in map and sort them (integers)\n    # This ensures 34, 36, 35 becomes 34, 35, 36\n    valid_segments = []\n    for seg_id in raw_segment_ids:\n        # Handle key types (str vs int) depending on your clip_map keys\n        details = clip_map.get(seg_id) or clip_map.get(str(seg_id))\n        if details:\n            valid_segments.append({\n                'id': int(seg_id),\n                'start': details['start'],\n                'end': details['end']\n            })\n\n    # Sort by ID to detect sequential numbers (34, 35, 36)\n    # valid_segments.sort(key=lambda x: x['id'])\n\n    if not valid_segments:\n        continue\n\n    # --- STEP 1: MERGE CONSECUTIVE SEGMENTS ---\n    merged_blocks = []\n\n    # Initialize the first block\n    current_block = {\n        'start': valid_segments[0]['start'],\n        'end': valid_segments[0]['end'],\n        'last_id': valid_segments[0]['id']\n    }\n\n    for i in range(1, len(valid_segments)):\n        seg = valid_segments[i]\n        prev_id = current_block['last_id']\n\n        # Check if this segment is exactly the next number (consecutive)\n        if seg['id'] == prev_id + 1:\n            # Update the end time of the current block\n            current_block['end'] = seg['end']\n            current_block['last_id'] = seg['id']\n            print(f'segment is next number, merge... id: {seg[\"id\"]}, prevId: {prev_id}')\n        else:\n            # Sequence broken, save current block and start new one\n            merged_blocks.append(current_block)\n            current_block = {\n                'start': seg['start'],\n                'end': seg['end'],\n                'last_id': seg['id']\n            }\n\n    # Append the final block\n    merged_blocks.append(current_block)\n\n    # --- STEP 2: ADD PADDING ---\n    final_segments = []\n\n    for i in range(len(merged_blocks)):\n        block = merged_blocks[i]\n\n        # If this is NOT the last block, check the gap to the next one\n        if i < len(merged_blocks) - 1:\n            next_block = merged_blocks[i+1]\n            gap = next_block['start'] - block['end']\n\n            # Logic: If gap is larger than padding config, extend current end time\n            if gap > PADDING_DURATION:\n                block['end'] += PADDING_DURATION\n\n        # Remove internal helper key 'last_id' before saving\n        final_segments.append({\n            'start': block['start'],\n            'end': block['end']\n        })\n\n    formatted_clips.append({\n        'title': title,\n        'description': description,\n        'segments': final_segments\n    })\n\n# Output Logic\noutput_clips_json_path = clips_json_path\nwith open(output_clips_json_path, 'w', encoding='utf-8') as f:\n    json.dump(formatted_clips, f, indent=2, ensure_ascii=False)\n\nprint(f\"Formatted clips saved to {output_clips_json_path}\")\nprint(json.dumps(formatted_clips, indent=2, ensure_ascii=False))","metadata":{"id":"0jp1e-Budcz7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\nimport subprocess\nimport re\nimport os\nimport sys\nimport json\n\ndef sanitize_filename(name):\n    \"\"\"\n    Sanitizes a string to be safe for filenames.\n    Replaces non-alphanumeric characters with underscores and converts to lowercase.\n    \"\"\"\n    # Replace anything that isn't a letter or number with _\n    s = re.sub(r'[^a-z0-9]', '_', name.lower(), flags=re.IGNORECASE)\n    # Remove duplicate underscores\n    return re.sub(r'_+', '_', s).strip('_')\n\ndef process_clips(json_path, video_path, output_dir=\"output\"):\n    # 1. Validate inputs\n    if not os.path.exists(json_path):\n        print(f\"Error: JSON file not found at '{json_path}'\")\n        return\n    if not os.path.exists(video_path):\n        print(f\"Error: Video file not found at '{video_path}'\")\n        return\n\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        print(f\"Created output directory: {output_dir}\")\n\n    # 2. Load JSON Data\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            clips_data = json.load(f)\n    except json.JSONDecodeError as e:\n        print(f\"Error decoding JSON: {e}\")\n        return\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return\n\n    if not isinstance(clips_data, list):\n        print(\"Error: JSON root must be a list of clips (array).\")\n        return\n\n    print(f\"\\nFound {len(clips_data)} clips to process.\")\n    print(\"=\" * 60)\n\n    # 3. Process each clip\n    for idx, clip in enumerate(clips_data):\n        title = clip.get('title', f\"clip_{idx+1}\")\n        safe_title = sanitize_filename(title)\n\n        # Ensure unique filenames if titles are identical\n        output_filename = os.path.join(output_dir, f\"{safe_title}.mp4\")\n\n        segments = clip.get('segments', [])\n        if not segments:\n            print(f\"Skipping '{title}': No segments found.\")\n            continue\n\n        print(f\"Processing [{idx+1}/{len(clips_data)}]: {title}\")\n        print(f\"  > Output: {output_filename}\")\n        print(f\"  > Segments: {len(segments)}\")\n\n        # Build Complex Filter\n        # This matches the React app logic exactly\n        filter_complex = \"\"\n        inputs = \"\"\n\n        for i, seg in enumerate(segments):\n            start = seg.get('start')\n            end = seg.get('end')\n\n            # Safety check\n            if start is None or end is None:\n                print(f\"    Warning: Skipping invalid segment {seg}\")\n                continue\n\n            # [0:v]trim=start=X:end=Y,setpts=PTS-STARTPTS[v0];\n            filter_complex += f\"[0:v]trim=start={start}:end={end},setpts=PTS-STARTPTS[v{i}];\"\n            # [0:a]atrim=start=X:end=Y,asetpts=PTS-STARTPTS[a0];\n            filter_complex += f\"[0:a]atrim=start={start}:end={end},asetpts=PTS-STARTPTS[a{i}];\"\n\n            inputs += f\"[v{i}][a{i}]\"\n\n        # Concat logic\n        # ...concat=n={count}:v=1:a=1[outv][outa]\n        filter_complex += f\"{inputs}concat=n={len(segments)}:v=1:a=1[outv][outa]\"\n\n        # Construct full command list (safer for subprocess)\n        # -y overwrites output files without asking\n        cmd = [\n            \"ffmpeg\",\n            \"-y\",\n            \"-i\", video_path,\n            \"-filter_complex\", filter_complex,\n            \"-map\", \"[outv]\",\n            \"-map\", \"[outa]\",\n            \"-c:v\", \"libx264\", # Re-encode video (essential for frame accuracy)\n            \"-crf\", \"23\",      # Quality (lower is better, 23 is standard)\n            \"-preset\", \"fast\", # Encoding speed\n            \"-c:a\", \"aac\",     # Re-encode audio\n            output_filename\n        ]\n\n        # Log the command (joined as string for easy copy-pasting if needed)\n        # We quote arguments that contain spaces for display purposes\n        full_c11md_str = \" \".join(f'\"{c}\"' if \" \" in str(c) or \"(\" in str(c) else str(c) for c in cmd)\n        print(f\"  > Command generated.\")\n\n        # 4. Run the command\n        try:\n            print(\"  > Running FFmpeg... \", end=\"\", flush=True)\n\n            # Run command, capture output to handle errors cleanly\n            result = subprocess.run(cmd, capture_output=True, text=True)\n\n            if result.returncode == 0:\n                print(\"Done!\")\n            else:\n                print(\"Failed!\")\n                print(f\"\\n[FFmpeg Error Log]:\\n{result.stderr}\")\n\n        except FileNotFoundError:\n             print(\"\\nError: 'ffmpeg' command not found. Please ensure FFmpeg is installed and in your system PATH.\")\n             sys.exit(1)\n        except Exception as e:\n            print(f\"\\nAn error occurred while running FFmpeg: {e}\")\n\n        print(\"-\" * 60)\n\n    print(\"\\nAll processing complete.\")\n","metadata":{"id":"8EEYdp8hHM8k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\nimport subprocess\nimport re\nimport os\nimport sys\nimport json\nimport shutil\n\ndef sanitize_filename(name):\n    \"\"\"Sanitizes a string to be safe for filenames.\"\"\"\n    s = re.sub(r'[^a-z0-9]', '_', name.lower(), flags=re.IGNORECASE)\n    return re.sub(r'_+', '_', s).strip('_')\n\ndef check_gpu():\n    \"\"\"Checks if NVIDIA GPU is available via nvidia-smi.\"\"\"\n    if shutil.which(\"nvidia-smi\") is None:\n        return False\n    try:\n        # Run nvidia-smi to query the GPU\n        subprocess.check_call([\"nvidia-smi\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\ndef get_encoding_settings(use_gpu):\n    \"\"\"Returns FFmpeg flags based on CPU or GPU mode.\"\"\"\n    if use_gpu:\n        print(\"  > Mode: GPU (NVIDIA NVENC)\")\n        return {\n            'input_flags': [\"-hwaccel\", \"cuda\"], # Decode on GPU\n            'video_codec': \"h264_nvenc\",         # Encode on GPU\n            'quality_flags': [\"-cq\", \"23\"],      # Constant Quality (GPU equivalent of CRF)\n            'preset': [\"-preset\", \"p5\"]          # p1 (fast) to p7 (slow/best)\n        }\n    else:\n        print(\"  > Mode: CPU (libx264)\")\n        return {\n            'input_flags': [],\n            'video_codec': \"libx264\",\n            'quality_flags': [\"-crf\", \"23\"],     # Constant Rate Factor\n            'preset': [\"-preset\", \"fast\"]\n        }\n\ndef process_clips(json_path, video_path, output_dir=\"output\", force_cpu=False):\n    output_files = []\n    # 1. Detect Hardware\n    gpu_available = check_gpu()\n\n    # Logic: Use GPU if available AND not forced to CPU\n    use_gpu = gpu_available and not force_cpu\n\n    # 2. Validate inputs\n    if not os.path.exists(json_path):\n        print(f\"Error: JSON file not found at '{json_path}'\")\n        return\n    if not os.path.exists(video_path):\n        print(f\"Error: Video file not found at '{video_path}'\")\n        return\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        print(f\"Created output directory: {output_dir}\")\n\n    # 3. Load JSON\n    try:\n        with open(json_path, 'r', encoding='utf-8') as f:\n            clips_data = json.load(f)\n    except Exception as e:\n        print(f\"Error reading JSON: {e}\")\n        return\n\n    if not isinstance(clips_data, list):\n        print(\"Error: JSON root must be a list of clips.\")\n        return\n\n    print(f\"\\nFound {len(clips_data)} clips to process.\")\n    print(\"=\" * 60)\n\n    # 4. Get Encoder Settings\n    settings = get_encoding_settings(use_gpu)\n\n    # 5. Process clips\n    for idx, clip in enumerate(clips_data):\n        title = clip.get('title', f\"clip_{idx+1}\")\n        safe_title = sanitize_filename(title)\n        output_filename = os.path.join(output_dir, f\"{safe_title}.mp4\")\n        output_files.append(output_filename)\n\n        segments = clip.get('segments', [])\n        if not segments:\n            continue\n\n        print(f\"Processing [{idx+1}/{len(clips_data)}]: {title}\")\n        print(f\"  > Output: {output_filename}\")\n\n        # Build Filter\n        filter_complex = \"\"\n        inputs = \"\"\n        for i, seg in enumerate(segments):\n            start, end = seg.get('start'), seg.get('end')\n            if start is None or end is None: continue\n\n            filter_complex += f\"[0:v]trim=start={start}:end={end},setpts=PTS-STARTPTS[v{i}];\"\n            filter_complex += f\"[0:a]atrim=start={start}:end={end},asetpts=PTS-STARTPTS[a{i}];\"\n            inputs += f\"[v{i}][a{i}]\"\n\n        filter_complex += f\"{inputs}concat=n={len(segments)}:v=1:a=1[outv][outa]\"\n\n        # Build Command\n        cmd = [\"ffmpeg\", \"-y\"]\n\n        # Add Input flags (hwaccel)\n        cmd.extend(settings['input_flags'])\n\n        cmd.extend([\"-i\", video_path])\n        cmd.extend([\"-filter_complex\", filter_complex])\n        cmd.extend([\"-map\", \"[outv]\", \"-map\", \"[outa]\"])\n\n        # Add Video Codec Settings\n        cmd.extend([\"-c:v\", settings['video_codec']])\n        cmd.extend(settings['quality_flags'])\n        cmd.extend(settings['preset'])\n\n        # Audio Codec (CPU is fine for audio)\n        cmd.extend([\"-c:a\", \"aac\"])\n\n        cmd.append(output_filename)\n\n        # Run\n        try:\n            print(\"  > Running FFmpeg... \", end=\"\", flush=True)\n            print(cmd)\n            result = subprocess.run(cmd, capture_output=True, text=True)\n            if result.returncode == 0:\n                print(\"Done!\")\n            else:\n                print(\"Failed!\")\n                # Print only the last few lines of error to keep it clean\n                print(f\"Error Log (Tail): {result.stderr[-500:]}\")\n        except Exception as e:\n            print(f\"\\nExecution error: {e}\")\n\n        print(\"-\" * 60)\n    print(\"Processing complete.\")\n    return output_files\n","metadata":{"id":"OLUd7Z3JT6SC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     parser = argparse.ArgumentParser(description=\"Auto-generate viral clips from AssemblyAI JSON using FFmpeg.\")\n#     parser.add_argument(\"json_file\", help=\"Path to the analysis JSON file\")\n#     parser.add_argument(\"video_file\", help=\"Path to the source MP4 video file\")\n#     parser.add_argument(\"--output\", \"-o\", default=\"viral_clips_output\", help=\"Directory to store output clips\")\n\n#     args = parser.parse_args()\n\n#     process_clips(args.json_file, args.video_file, args.output)","metadata":{"id":"NvdYO_RMLafo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Running","metadata":{"id":"HNDtKeExPXy0"}},{"cell_type":"code","source":"output_files = process_clips(clips_json_path, video_file_path, output_base_path)","metadata":{"id":"HXOTZ_dELjmC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Uploads","metadata":{}},{"cell_type":"code","source":"if len(output_files) == 0:\n    uploader.send_text_message(\"--- No Clip Found ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"uploader.process_files(output_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"uploader.send_text_message(editing_instructions, \"Markdown\")\nuploader.send_text_message(f\"source: {youtube_url}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Editing Video","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom moviepy.editor import VideoFileClip\nfrom ultralytics import YOLO\n\n# --- CONFIGURATION ---\nINPUT_VIDEO = \"podcast_landscape.mp4\"\nOUTPUT_VIDEO = \"podcast_short_yolo.mp4\"\nSMOOTHING_WINDOW = 45  # Higher = smoother camera movement\nYOLO_MODEL = \"yolov8n.pt\"  # 'n' is nano (fastest), 's' is small (more accurate)\n\ndef process_video_yolo(input_path, output_path):\n    print(\"Loading YOLO model...\")\n    # This will download the model automatically on first run\n    model = YOLO(YOLO_MODEL)\n    \n    print(\"Loading video...\")\n    clip = VideoFileClip(input_path)\n    w, h = clip.size\n    \n    # Calculate target dimensions for 9:16\n    # Example: If height is 1080, width becomes ~607\n    target_ratio = 9 / 16\n    new_w = int(h * target_ratio) \n    new_h = h\n\n    print(\"Analyzing frames (this may take a moment)...\")\n    \n    x_centers = []\n    \n    # Iterate through frames\n    # Note: For testing speed, you can iterate every 5th frame and interpolate later\n    # But for quality, we check every frame here.\n    for frame in clip.iter_frames():\n        # YOLO expects RGB, MoviePy provides RGB\n        # We limit detection to class 0 ('person') to avoid detecting cups, chairs, etc.\n        results = model(frame, classes=[0], verbose=False)\n        \n        # Default to center of video if no one is found\n        frame_center_x = w / 2\n        detected_x = frame_center_x\n\n        if len(results[0].boxes) > 0:\n            # Find the largest person (closest to camera/main speaker)\n            # boxes.xyxy returns [x1, y1, x2, y2]\n            best_box = None\n            max_area = 0\n\n            for box in results[0].boxes.xyxy:\n                x1, y1, x2, y2 = box.cpu().numpy()\n                area = (x2 - x1) * (y2 - y1)\n                if area > max_area:\n                    max_area = area\n                    best_box = (x1, x2)\n            \n            if best_box:\n                x1, x2 = best_box\n                # Calculate the horizontal center of the person\n                detected_x = int((x1 + x2) / 2)\n        \n        # Logic to handle if detection is lost momentarily:\n        # If we have previous data, stay there. If not, go to center.\n        if detected_x == frame_center_x and len(x_centers) > 0:\n             x_centers.append(x_centers[-1])\n        else:\n             x_centers.append(detected_x)\n\n    print(f\"Analyzed {len(x_centers)} frames.\")\n    print(\"Smoothing camera movement...\")\n\n    # --- SMOOTHING ALGORITHM ---\n    # This prevents the camera from shaking violently every time the person moves an inch\n    def moving_average(data, window_size):\n        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n\n    # Pad the data so the array length stays the same after smoothing\n    pad_width = SMOOTHING_WINDOW // 2\n    padded_centers = np.pad(x_centers, (pad_width, pad_width), mode='edge')\n    smoothed_centers = moving_average(padded_centers, SMOOTHING_WINDOW)\n    \n    # Trim to match exact frame count\n    smoothed_centers = smoothed_centers[:len(x_centers)]\n\n    print(\"Rendering final video...\")\n\n    def get_crop_params(t):\n        frame_index = int(t * clip.fps)\n        frame_index = min(frame_index, len(smoothed_centers) - 1)\n        \n        current_center_x = smoothed_centers[frame_index]\n        \n        # Calculate crop coordinates (Left, Top, Right, Bottom)\n        x1 = int(current_center_x - (new_w / 2))\n        \n        # Boundary checks (Don't crop outside the video)\n        if x1 < 0: x1 = 0\n        if x1 + new_w > w: x1 = w - new_w\n        \n        return x1, 0, x1 + new_w, new_h\n\n    # Apply the dynamic crop using MoviePy's special filter\n    final_clip = clip.fl(lambda gf, t: gf(t)[0:new_h, get_crop_params(t)[0]:get_crop_params(t)[2]], keep_duration=True)\n\n    # Export\n    # preset='fast' speeds up encoding, use 'slow' for slightly smaller file size\n    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=clip.fps, preset='fast')\n\n    print(\"Done! Video saved to\", output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom moviepy.editor import VideoFileClip\nfrom ultralytics import YOLO\n\n# --- CONFIGURATION ---\nINPUT_VIDEO = \"podcast_landscape.mp4\"\nOUTPUT_VIDEO = \"podcast_short_stable.mp4\"\n\n# Increased window for buttery smooth transitions\nSMOOTHING_WINDOW = 30  \n\n# New setting: How many pixels the person must move before the camera follows.\n# 50 to 100 is usually good for 1080p video.\nDEAD_ZONE_LIMIT = 120 \n\nYOLO_MODEL = \"yolov8n.pt\"\n\ndef process_video_stable(input_path, output_path):\n    print(\"Loading YOLO model...\")\n    model = YOLO(YOLO_MODEL)\n    \n    print(\"Loading video...\")\n    clip = VideoFileClip(input_path)\n    w, h = clip.size\n    \n    # Target 9:16\n    new_w = int(h * (9/16)) \n    new_h = h\n\n    # If width is odd (e.g., 607), make it even (606)\n    if new_w % 2 != 0:\n        new_w -= 1\n\n    print(\"Analyzing frames...\")\n    \n    # We store the RAW detections here\n    raw_centers = []\n    \n    # Iterate frames\n    for frame in clip.iter_frames():\n        results = model(frame, classes=[0], verbose=False)\n        \n        # Default to center if nothing found\n        detected_x = w / 2\n\n        if len(results[0].boxes) > 0:\n            # Find largest person\n            best_box = None\n            max_area = 0\n            for box in results[0].boxes.xyxy:\n                x1, y1, x2, y2 = box.cpu().numpy()\n                area = (x2 - x1) * (y2 - y1)\n                if area > max_area:\n                    max_area = area\n                    best_box = (x1, x2)\n            \n            if best_box:\n                x1, x2 = best_box\n                detected_x = int((x1 + x2) / 2)\n        \n        # Fill missing data with previous frame or center\n        if detected_x == w/2 and len(raw_centers) > 0:\n             raw_centers.append(raw_centers[-1])\n        else:\n             raw_centers.append(detected_x)\n\n    print(f\"Analyzed {len(raw_centers)} frames.\")\n    print(\"Applying Dead Zone & Smoothing...\")\n\n    # --- STEP 1: APPLY DEAD ZONE ---\n    # This turns shaky data into \"steps\". The camera stays locked\n    # until the subject moves outside the threshold.\n    stabilized_centers = []\n    last_anchor = raw_centers[0]\n    \n    for current_x in raw_centers:\n        # Distance between where the camera IS and where the person IS\n        diff = abs(current_x - last_anchor)\n        \n        if diff < DEAD_ZONE_LIMIT:\n            # Movement is too small (jitters/breathing), ignore it\n            stabilized_centers.append(last_anchor)\n        else:\n            # Movement is large, update the anchor\n            # (We use 0.95 factor to slowly drift rather than snap, optional)\n            last_anchor = current_x\n            stabilized_centers.append(last_anchor)\n\n    # --- STEP 2: APPLY SMOOTHING ---\n    # Now we smooth the \"steps\" created above so they look like pans\n    def moving_average(data, window_size):\n        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n\n    pad_width = SMOOTHING_WINDOW // 2\n    padded_centers = np.pad(stabilized_centers, (pad_width, pad_width), mode='edge')\n    final_centers = moving_average(padded_centers, SMOOTHING_WINDOW)\n    final_centers = final_centers[:len(raw_centers)]\n\n    print(\"Rendering final video...\")\n\n    def get_crop_params(t):\n        frame_index = int(t * clip.fps)\n        frame_index = min(frame_index, len(final_centers) - 1)\n        \n        current_center_x = final_centers[frame_index]\n        \n        x1 = int(current_center_x - (new_w / 2))\n        \n        # Clamp to bounds\n        if x1 < 0: x1 = 0\n        if x1 + new_w > w: x1 = w - new_w\n        \n        return x1, 0, x1 + new_w, new_h\n\n    final_clip = clip.fl(lambda gf, t: gf(t)[0:new_h, get_crop_params(t)[0]:get_crop_params(t)[2]], keep_duration=True)\n    \n    # Using 'preset=ultrafast' for testing speed, change to 'medium' for final quality\n    # final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", preset='medium')\n     # 2. FIX EXPORT (Force Pixel Format for Mac Compatibility)\n    final_clip.write_videofile(\n        output_path, \n        codec=\"libx264\", \n        audio_codec=\"aac\", \n        \n        # THIS IS THE CRITICAL LINE FOR MAC/QUICKTIME:\n        ffmpeg_params=['-pix_fmt', 'yuv420p'], \n        \n        preset='medium'\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom moviepy.editor import VideoFileClip\nfrom ultralytics import YOLO\nfrom sklearn.cluster import KMeans\n\n# --- CONFIGURATION ---\nINPUT_VIDEO = \"podcast_landscape.mp4\"\nOUTPUT_VIDEO = \"podcast_short_stable.mp4\"\nYOLO_MODEL = \"yolov8n.pt\"\n\n# --- EDITING RULES ---\n# Minimum time (in seconds) the camera must stay on a person before switching.\n# Prevents the \"Zig-Zag\" effect.\nMIN_SHOT_DURATION = 2.0  \n\n# If true, the camera locks perfectly still on the person's \"seat\".\n# If false, it follows them slightly. True is better for professional look.\nLOCK_CAMERA_TO_SEAT = True\n\ndef process_video_smart(input_path, output_path):\n    print(\"Loading YOLO model...\")\n    model = YOLO(YOLO_MODEL)\n    \n    print(\"Loading video...\")\n    clip = VideoFileClip(input_path)\n    fps = clip.fps\n    w, h = clip.size\n    duration_frames = int(clip.duration * fps)\n    \n    # Target 9:16 Dimensions\n    new_w = int(h * (9/16)) \n    new_h = h\n    if new_w % 2 != 0: new_w -= 1 # Ensure width is even\n\n    print(\"Step 1: Analyzing Frames (This may take time)...\")\n    \n    # Store (frame_index, center_x_of_largest_face)\n    detections = [] \n    \n    # We iterate through the video to gather data first\n    for i, frame in enumerate(clip.iter_frames()):\n        results = model(frame, classes=[0], verbose=False) # class 0 = person\n        \n        best_center = None\n        max_area = 0\n        \n        # Find the dominant person in the frame\n        if len(results[0].boxes) > 0:\n            for box in results[0].boxes.xyxy:\n                x1, y1, x2, y2 = box.cpu().numpy()\n                area = (x2 - x1) * (y2 - y1)\n                \n                # Heuristic: Largest face is likely the active speaker/subject\n                if area > max_area:\n                    max_area = area\n                    best_center = int((x1 + x2) / 2)\n        \n        detections.append(best_center)\n\n        if i % 100 == 0:\n            print(f\"Scanned {i}/{duration_frames} frames...\")\n\n    print(\"Step 2: Calculating 'Seat' Positions...\")\n    \n    # Filter out None values to find the clusters\n    valid_centers = [x for x in detections if x is not None]\n    valid_centers_reshaped = np.array(valid_centers).reshape(-1, 1)\n\n    # Use K-Means to find the 2 main \"Sitting Positions\" (Left Person, Right Person)\n    # This removes jitter because we won't rely on the exact face pixel every frame\n    kmeans = KMeans(n_clusters=2, n_init=10).fit(valid_centers_reshaped)\n    seat_locations = sorted(kmeans.cluster_centers_.flatten())\n    \n    print(f\"Detected Seat Locations (Camera Angles): {seat_locations}\")\n\n    print(\"Step 3: Director Logic (Deciding when to cut)...\")\n    \n    final_camera_positions = []\n    \n    current_seat = seat_locations[0] # Start at first seat\n    frames_since_cut = 0\n    min_frames_wait = int(MIN_SHOT_DURATION * fps)\n\n    # Fill in None detections with previous known location\n    filled_detections = []\n    last_known = seat_locations[0]\n    for d in detections:\n        if d is not None:\n            last_known = d\n        filled_detections.append(last_known)\n\n    for i in range(len(filled_detections)):\n        detected_x = filled_detections[i]\n        \n        # Find which \"Seat\" this detection is closest to\n        closest_seat = min(seat_locations, key=lambda seat: abs(seat - detected_x))\n        \n        # LOGIC: Should we switch camera?\n        # 1. Is the detected person different from what we are showing?\n        # 2. Have we stayed on the current shot long enough?\n        if closest_seat != current_seat and frames_since_cut > min_frames_wait:\n            current_seat = closest_seat\n            frames_since_cut = 0 # Reset timer\n        else:\n            frames_since_cut += 1\n        \n        # Determine final X coordinate for this frame\n        if LOCK_CAMERA_TO_SEAT:\n            # OPTION A: Camera on Tripod (Professional, no shake)\n            final_camera_positions.append(current_seat)\n        else:\n            # OPTION B: Handheld feel (Follows face loosely, but snaps to new person)\n            # Only use detected_x if it matches the current active seat\n            if abs(detected_x - current_seat) < 200: \n                final_camera_positions.append(detected_x)\n            else:\n                final_camera_positions.append(current_seat)\n\n    print(\"Step 4: Rendering...\")\n\n    # Define the crop function for MoviePy\n    def get_crop(t):\n        frame_idx = int(t * fps)\n        # Safety check for index out of bounds\n        frame_idx = min(frame_idx, len(final_camera_positions) - 1)\n        \n        center_x = final_camera_positions[frame_idx]\n        \n        # Calculate top-left corner (x1, y1)\n        x1 = int(center_x - (new_w / 2))\n        \n        # Boundary checks (Don't crop outside the video)\n        if x1 < 0: x1 = 0\n        if x1 + new_w > w: x1 = w - new_w\n        \n        return x1, 0, x1 + new_w, new_h\n\n    # Apply the crop\n    final_clip = clip.fl(lambda gf, t: gf(t)[0:new_h, get_crop(t)[0]:get_crop(t)[2]], keep_duration=True)\n\n    # Write file\n    final_clip.write_videofile(\n        output_path, \n        codec=\"libx264\", \n        audio_codec=\"aac\", \n        ffmpeg_params=['-pix_fmt', 'yuv420p'], \n        preset='medium'\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# process_video_smart('/kaggle/input/video-podcast-samples/kerugian_300_milyar_skandal_akademi_crypto.mp4', 'output2.mp4')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom moviepy.editor import VideoFileClip, clips_array, concatenate_videoclips\nfrom ultralytics import YOLO\nfrom sklearn.cluster import KMeans\n\n# --- CONFIG ---\nINPUT_VIDEO = \"podcast_landscape.mp4\"\nOUTPUT_VIDEO = \"podcast_dynamic.mp4\"\n\n# If the director wants to switch cameras more than X times in Y seconds,\n# we force a split screen.\nTENSION_WINDOW = 4.0 # seconds\nMAX_SWITCHES_ALLOWED = 2 # If more than 2 switches happen in 4 seconds -> SPLIT SCREEN\n\ndef process_dynamic_podcast(input_path, output_path):\n    model = YOLO(YOLO_MODEL)\n    clip = VideoFileClip(input_path)\n    fps = clip.fps\n    \n    # 1. ANALYZE & FIND SEATS (Same as before)\n    # ... (Code to find seat_locations using KMeans) ...\n    # Let's assume we found 3 seats for this example: [Left, Middle, Right]\n    # For now, I will use placeholder code for the detection part to save space\n    # You would use the exact KMeans code from the previous response here.\n    \n    # [PLACEHOLDER for Analysis Phase - assumes you have detections]\n    # detections = [center_x, center_x, ...] per frame\n    # seat_locations = [200, 900] (Example 2 seats)\n    \n    print(\"Simulating Director Logic...\", clip.duration)\n    \n    # 2. GENERATE \"VIRTUAL TIMELINE\"\n    # We don't render yet. We create a list of \"Shot Events\".\n    # Event format: {'start': 0.0, 'end': 2.5, 'seat': 200}\n    \n    raw_shots = []\n    current_seat = seat_locations[0]\n    last_switch_time = 0.0\n    \n    for t in np.arange(0, clip.duration, 1/fps):\n        # ... logic to determine who is speaking (Active Speaker) ...\n        # Let's say we determine 'target_seat' based on detection\n        \n        if target_seat != current_seat and (t - last_switch_time) > 2.0:\n            # Record the previous shot\n            raw_shots.append({\n                'start': last_switch_time,\n                'end': t,\n                'type': 'fullscreen',\n                'seats': [current_seat]\n            })\n            current_seat = target_seat\n            last_switch_time = t\n\n    # Append final shot\n    raw_shots.append({\n        'start': last_switch_time,\n        'end': clip.duration,\n        'type': 'fullscreen',\n        'seats': [current_seat]\n    })\n\n    # 3. THE TENSION MONITOR (The Magic Step)\n    # We look at the list of shots and merge rapid fire ones into Split Screens\n    \n    final_events = []\n    buffer = []\n    \n    for shot in raw_shots:\n        buffer.append(shot)\n        \n        # Calculate duration of the buffer\n        buffer_duration = buffer[-1]['end'] - buffer[0]['start']\n        \n        # If buffer is long enough, analyze it\n        if buffer_duration >= TENSION_WINDOW:\n            num_switches = len(buffer)\n            \n            if num_switches > MAX_SWITCHES_ALLOWED:\n                # HIGH TENSION DETECTED!\n                # Identify the 2 most active seats in this buffer\n                involved_seats = [s['seats'][0] for s in buffer]\n                # Find most common 2 seats (in case of 3 people)\n                from collections import Counter\n                most_common = Counter(involved_seats).most_common(2)\n                seat_A = most_common[0][0]\n                seat_B = most_common[1][0]\n                \n                # Merge buffer into ONE split screen event\n                final_events.append({\n                    'start': buffer[0]['start'],\n                    'end': buffer[-1]['end'],\n                    'type': 'split',\n                    'seats': [seat_A, seat_B] # Top, Bottom\n                })\n            else:\n                # LOW TENSION - Keep as separate full screen shots\n                final_events.extend(buffer)\n            \n            buffer = [] # Clear buffer\n\n    # 4. RENDER\n    print(\"Rendering Dynamic Timeline...\")\n    clips_list = []\n    \n    for event in final_events:\n        sub = clip.subclip(event['start'], event['end'])\n        \n        if event['type'] == 'fullscreen':\n            # Crop to the single seat\n            seat = event['seats'][0]\n            # ... Crop logic ...\n            cropped = sub # (Apply crop function here)\n            clips_list.append(cropped)\n            \n        elif event['type'] == 'split':\n            # Create Split Screen\n            seat_top = event['seats'][0]\n            seat_bottom = event['seats'][1]\n            \n            # ... Crop Logic Top ...\n            # ... Crop Logic Bottom ...\n            \n            stacked = clips_array([[clip_top], [clip_bottom]])\n            clips_list.append(stacked)\n\n    final_video = concatenate_videoclips(clips_list)\n    final_video.write_videofile(output_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom moviepy.editor import VideoFileClip, clips_array, concatenate_videoclips, vfx\nfrom ultralytics import YOLO\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\n\n# --- CONFIGURATION ---\nINPUT_VIDEO = \"podcast_landscape.mp4\"\nOUTPUT_VIDEO = \"podcast_dynamic_final.mp4\"\nYOLO_MODEL = \"yolov8n.pt\"\n\n# --- RULES ---\n# If cameras switch more than X times in Y seconds, force Split Screen\nTENSION_WINDOW_SECONDS = 4.0 \nMAX_SWITCHES_ALLOWED = 2 \n\n# Minimum time to stay on one person in Full Screen (prevents flickering)\nMIN_SHOT_DURATION = 1.5 \n\ndef process_dynamic_podcast(input_path, output_path):\n    # ---------------------------------------------------------\n    # STEP 1: INITIALIZATION & DETECTION (The \"Eyes\")\n    # ---------------------------------------------------------\n    print(\"Loading YOLO and Video...\")\n    model = YOLO(YOLO_MODEL)\n    clip = VideoFileClip(input_path)\n    fps = clip.fps\n    w, h = clip.size\n    duration_frames = int(clip.duration * fps)\n    \n    # Target 9:16 Dimensions for the final output\n    target_h = h\n    target_w = int(target_h * (9/16))\n    if target_w % 2 != 0: target_w -= 1 # Width must be even for encoding\n\n    print(\"Analyzing Frames for Human Positions...\")\n    detections = [] \n    \n    # Run YOLO on frames (Same as previous version)\n    for i, frame in enumerate(clip.iter_frames()):\n        results = model(frame, classes=[0], verbose=False) # class 0 = person\n        best_center = None\n        max_area = 0\n        \n        if len(results[0].boxes) > 0:\n            for box in results[0].boxes.xyxy:\n                x1, y1, x2, y2 = box.cpu().numpy()\n                area = (x2 - x1) * (y2 - y1)\n                # Assume largest box is the active speaker\n                if area > max_area:\n                    max_area = area\n                    best_center = int((x1 + x2) / 2)\n        \n        detections.append(best_center)\n        if i % 100 == 0: print(f\"Scanned {i}/{duration_frames}...\")\n\n    # ---------------------------------------------------------\n    # STEP 2: CLUSTERING (The \"Brain\")\n    # ---------------------------------------------------------\n    print(\"Calculating 'Seat' Locations...\")\n    \n    # Filter valid detections\n    valid_centers = [x for x in detections if x is not None]\n    if not valid_centers:\n        raise ValueError(\"No humans detected in video!\")\n\n    valid_centers_reshaped = np.array(valid_centers).reshape(-1, 1)\n\n    # Find the 2 main seats (Left Person, Right Person)\n    kmeans = KMeans(n_clusters=2, n_init=10).fit(valid_centers_reshaped)\n    seat_locations = sorted(kmeans.cluster_centers_.flatten())\n    print(f\"Defined Seat X-Coordinates: {seat_locations}\")\n\n    # ---------------------------------------------------------\n    # STEP 3: GENERATE RAW TIMELINE (The \"Rough Cut\")\n    # ---------------------------------------------------------\n    print(\"Generating Rough Cut Timeline...\")\n    \n    raw_shots = []\n    \n    # Helper to find closest seat to a detection\n    def get_closest_seat(x_pos):\n        return min(seat_locations, key=lambda seat: abs(seat - x_pos))\n\n    # Fill None values in detections with last known position\n    filled_detections = []\n    last_known = seat_locations[0]\n    for d in detections:\n        if d is not None: last_known = d\n        filled_detections.append(last_known)\n\n    # Loop to create shots\n    current_seat = get_closest_seat(filled_detections[0])\n    shot_start_frame = 0\n    min_frames = int(MIN_SHOT_DURATION * fps)\n\n    for i in range(len(filled_detections)):\n        detected_seat = get_closest_seat(filled_detections[i])\n        \n        # Determine if we should cut\n        frames_since_cut = i - shot_start_frame\n        \n        # If person changed AND we have waited long enough\n        if detected_seat != current_seat and frames_since_cut > min_frames:\n            # End previous shot\n            raw_shots.append({\n                'start': shot_start_frame / fps,\n                'end': i / fps,\n                'type': 'fullscreen',\n                'seats': [current_seat]\n            })\n            # Start new shot\n            current_seat = detected_seat\n            shot_start_frame = i\n\n    # Append the final remaining shot\n    raw_shots.append({\n        'start': shot_start_frame / fps,\n        'end': clip.duration,\n        'type': 'fullscreen',\n        'seats': [current_seat]\n    })\n\n    # ---------------------------------------------------------\n    # STEP 4: TENSION MONITOR (The \"Magic\")\n    # ---------------------------------------------------------\n    print(\"Running Tension Monitor (Detecting Rapid Switches)...\")\n    \n    final_events = []\n    buffer = []\n    \n    for shot in raw_shots:\n        buffer.append(shot)\n        \n        # Calculate total duration of the shots currently in buffer\n        buffer_start = buffer[0]['start']\n        buffer_end = buffer[-1]['end']\n        buffer_duration = buffer_end - buffer_start\n        \n        # Once buffer exceeds window size, analyze it\n        if buffer_duration >= TENSION_WINDOW_SECONDS:\n            num_shots = len(buffer)\n            \n            # LOGIC: If many cuts happened in this short window -> SPLIT SCREEN\n            if num_shots > MAX_SWITCHES_ALLOWED:\n                print(f\"High Tension Detected at {buffer_start:.2f}s! Switching to Split Screen.\")\n                \n                # Identify the two seats involved\n                seat_A = seat_locations[0] # Left Person\n                seat_B = seat_locations[1] # Right Person\n                \n                final_events.append({\n                    'start': buffer_start,\n                    'end': buffer_end,\n                    'type': 'split',\n                    'seats': [seat_A, seat_B]\n                })\n            else:\n                # Normal pacing -> Keep original Full Screen shots\n                final_events.extend(buffer)\n            \n            buffer = [] # Clear buffer\n\n    # Add any remaining buffer\n    if buffer:\n        final_events.extend(buffer)\n\n    # ---------------------------------------------------------\n    # STEP 5: RENDERING (The \"Output\")\n    # ---------------------------------------------------------\n    print(f\"Rendering {len(final_events)} clips...\")\n    clips_list = []\n    \n    def crop_to_vertical(video_clip, center_x):\n        # Calculate crop box\n        x1 = int(center_x - (target_w / 2))\n        # Bounds check\n        if x1 < 0: x1 = 0\n        if x1 + target_w > w: x1 = w - target_w\n        return video_clip.crop(x1=x1, y1=0, x2=x1+target_w, y2=target_h)\n\n    for event in final_events:\n        # Create the subclip for this specific time\n        sub = clip.subclip(event['start'], event['end'])\n        \n        if event['type'] == 'fullscreen':\n            # Crop to the active speaker\n            cropped = crop_to_vertical(sub, event['seats'][0])\n            clips_list.append(cropped)\n            \n        elif event['type'] == 'split':\n            # Create Top/Bottom Split\n            seat_top_x = event['seats'][0] \n            seat_bottom_x = event['seats'][1]\n            \n            # We need to squish or crop? \n            # Better approach: Crop two full vertical videos, then resize them \n            # to stack on top of each other.\n            \n            clip_top = crop_to_vertical(sub, seat_top_x)\n            clip_bottom = crop_to_vertical(sub, seat_bottom_x)\n            \n            # Resize height to 50% so they fit in one frame\n            clip_top = clip_top.resize(height=target_h // 2)\n            clip_bottom = clip_bottom.resize(height=target_h // 2)\n            \n            # Stack them\n            stacked = clips_array([[clip_top], [clip_bottom]])\n            clips_list.append(stacked)\n\n    # Concatenate all clips\n    final_video = concatenate_videoclips(clips_list)\n    \n    # Write output\n    final_video.write_videofile(\n        output_path, \n        codec=\"libx264\", \n        audio_codec=\"aac\", \n        ffmpeg_params=['-pix_fmt', 'yuv420p'], \n        preset='medium'\n    )\n    print(\"Done!\")\n\n# --- RUN IT ---\n# process_dynamic_podcast(INPUT_VIDEO, OUTPUT_VIDEO)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# process_dynamic_podcast('/kaggle/input/video-podcast-samples/kerugian_300_milyar_skandal_akademi_crypto.mp4', 'output_d.mp4')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# process_video_stable(\"/kaggle/input/viral-clip-processor/2025-12-20-Untitled_List/skill_5_tahun_lagi_bukan_ai_tapi_etika.mp4\", \"result1.mp4\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncrop_files = []\nfor clip_path in output_files:\n    crop_path = clip_path.replace(\".mp4\", \"_crop.mp4\")\n    # process_video_stable(clip_path, crop_path)\n    process_video_smart(clip_path, crop_path)\n    crop_files.append(crop_path)\n\n\nuploader.process_files(crop_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Burn Subtitle","metadata":{}},{"cell_type":"code","source":"# --- FONT INSTALLATION ---\nimport os\nimport requests\n\ndef download_file(url, destination):\n    \"\"\"\n    Downloads a file from a URL to a local destination using the requests library.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        # Raise an exception for bad status codes (4XX or 5XX)\n        response.raise_for_status() \n\n        with open(destination, 'wb') as f:\n            f.write(response.content)\n        print(f\"File downloaded successfully to {destination}\")\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Download failed: {e}\")\n\n\nfont_url = \"https://subtitle-generator.annasblackhat.web.id/fonts/the_bold.ttf\"\nfont_filename = \"TheBoldFont.ttf\"  # <--- Change this to your actual filename\ndownload_file(font_url, font_filename)\n\nfont_path = f\"/usr/local/share/fonts/{font_filename}\"\n\nif os.path.exists(font_filename):\n    # Create the font directory if it doesn't exist\n    !mkdir -p /usr/local/share/fonts/\n    \n    # Move the font to the system font directory\n    !cp {font_filename} /usr/local/share/fonts/\n    \n    # Refresh the font cache (CRITICAL)\n    !fc-cache -fv\n    print(f\"Successfully installed {font_filename}\")\nelse:\n    print(f\"Warning: {font_filename} not found in current directory. Please upload it.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport subprocess\n\ndef format_ass_time(seconds):\n    total_centis = int(round(seconds * 100))\n    hours = total_centis // 360000\n    minutes = (total_centis % 360000) // 6000\n    secs = (total_centis % 6000) // 100\n    centis = total_centis % 100\n    return f\"{hours}:{minutes:02d}:{secs:02d}.{centis:02d}\"\n\ndef generate_ass(words, style):\n    header = f\"\"\"[Script Info]\nTitle: Generated by AI Subtitle Generator\nScriptType: v4.00+\nWrapStyle: 0\nScaledBorderAndShadow: yes\nYCbCr Matrix: None\n\n[V4+ Styles]\nFormat: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\nStyle: Default,{style['FontName']},{style['FontSize']},{style['PrimaryColour']},&H000000FF,{style.get('OutlineColour', '&H00000000')},{style.get('BackColour', '&H00000000')},0,0,0,0,100,100,0,0,{style['BorderStyle']},2,{style.get('Shadow', 0)},{style['Alignment']},10,10,{style['MarginV']},1\n\n[Events]\nFormat: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n\"\"\"\n    dialogue_lines = []\n    current_line_words = []\n    \n    def process_line(line_words):\n        if not line_words: return\n        \n        line_start = line_words[0]['start']\n        line_end = line_words[-1]['end']\n        \n        if 'ActiveFontSize' in style and 'ActivePrimaryColour' in style:\n            for idx, active_word in enumerate(line_words):\n                next_word = line_words[idx + 1] if idx + 1 < len(line_words) else None\n                event_end = next_word['start'] if (next_word and next_word['start'] > active_word['end']) else active_word['end']\n                \n                text_parts = []\n                for w in line_words:\n                    if w == active_word:\n                        text_parts.append(f\"{{\\\\1c{style['ActivePrimaryColour']}\\\\fs{style['ActiveFontSize']}}}{w['word']}{{\\\\1c{style['PrimaryColour']}\\\\fs{style['FontSize']}}}\")\n                    else:\n                        text_parts.append(w['word'])\n                \n                dialogue_text = ' '.join(text_parts)\n                dialogue = f\"Dialogue: 0,{format_ass_time(active_word['start'])},{format_ass_time(event_end)},Default,,0,0,0,,{dialogue_text}\"\n                dialogue_lines.append(dialogue)\n        else:\n            full_text = ' '.join([w['word'] for w in line_words])\n            dialogue = f\"Dialogue: 0,{format_ass_time(line_start)},{format_ass_time(line_end)},Default,,0,0,0,,{full_text}\"\n            dialogue_lines.append(dialogue)\n\n    pause_threshold = 0.2\n    for i, word in enumerate(words):\n        current_line_words.append(word)\n        \n        next_word = words[i+1] if i+1 < len(words) else None\n        is_last_word = i == len(words) - 1\n        word_count = len(current_line_words)\n        is_long_pause = next_word and (next_word['start'] - word['end'] > pause_threshold)\n        \n        if is_last_word or is_long_pause or word_count >= 3:\n            process_line(current_line_words)\n            current_line_words = []\n            \n    return header + '\\n'.join(dialogue_lines)\n\ndef get_words_for_clip(full_transcription, clip_segments):\n    all_words = []\n    for segment in full_transcription['segments']:\n        if 'words' in segment:\n            all_words.extend(segment['words'])\n    \n    clip_words = []\n    current_clip_time = 0\n    for seg in clip_segments:\n        seg_start = seg['start']\n        seg_end = seg['end']\n        seg_duration = seg_end - seg_start\n        \n        words_in_seg = [w for w in all_words if w['start'] >= seg_start and w['end'] <= seg_end]\n        \n        for w in words_in_seg:\n            shifted_word = {\n                'word': w['word'].strip(),\n                'start': w['start'] - seg_start + current_clip_time,\n                'end': w['end'] - seg_start + current_clip_time\n            }\n            clip_words.append(shifted_word)\n        \n        current_clip_time += seg_duration\n    \n    return clip_words\n\ndef burn_subtitles(video_input, ass_path, video_output):\n    # Escape colons in path for subtitles filter\n    safe_ass_path = ass_path.replace(':', '\\\\\\\\:')\n    cmd = [\n        \"ffmpeg\", \"-y\",\n        \"-i\", video_input,\n        \"-vf\", f\"subtitles={safe_ass_path}\",\n        \"-c:v\", \"libx264\",\n        \"-crf\", \"23\",\n        \"-preset\", \"fast\",\n        \"-c:a\", \"copy\",\n        video_output\n    ]\n    subprocess.run(cmd, check=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- SUBTITLE CONFIGURATION ---\nsubtitle_style = {\n    'FontName': 'THE BOLD FONT FREE VERSION',\n    'FontSize': 18,\n    'PrimaryColour': '&H00FFFFFF',\n    'OutlineColour': '&H00000000',\n    'BackColour': '&H80000000',\n    'BorderStyle': 1,\n    'Shadow': 0,\n    'Alignment': 2,\n    'MarginV': 60,\n    'ActiveFontSize': 24,\n    'ActivePrimaryColour': '&H0000FF00', # Green\n}\n\nfinal_output_files = []\n\nprint(f\"Generating subtitles for {len(crop_files)} clips...\")\n\nfor i, crop_path in enumerate(crop_files):\n    clip_data = formatted_clips[i]\n    clip_title = clip_data['title']\n    \n    print(f\"Processing subtitle for: {clip_title}\")\n    \n    # 1. Extract words for this specific clip\n    clip_words = get_words_for_clip(transcribe, clip_data['segments'])\n    \n    # 2. Generate ASS content\n    ass_content = generate_ass(clip_words, subtitle_style)\n    \n    # 3. Save ASS file\n    ass_path = crop_path.replace('.mp4', '.ass')\n    with open(ass_path, 'w', encoding='utf-8') as f:\n        f.write(ass_content)\n    \n    # 4. Burn subtitles\n    final_path = crop_path.replace('.mp4', '_final.mp4')\n    try:\n        burn_subtitles(crop_path, ass_path, final_path)\n        final_output_files.append(final_path)\n        print(f'  [+] Burned subtitles: {final_path}')\n    except Exception as e:\n        print(f'  [-] Failed to burn subtitles for {clip_title}: {e}')\n\nprint('Uploading final videos with subtitles...')\nuploader.process_files(final_output_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pillow-lut","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom PIL import Image\nfrom pillow_lut import load_cube_file\n\ndef apply_lut_to_video(input_video_path, output_video_path, lut):\n    # Open the input video\n    cap = cv2.VideoCapture(input_video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video {input_video_path}\")\n        return\n\n    # Get video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    frame_size = (width, height)\n\n    # Define the codec and create VideoWriter object\n    # Use 'mp4v' for .mp4 output. Adjust as needed for different formats.\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n    out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n\n    if not out.isOpened():\n        print(f\"Error: Could not create output video {output_video_path}\")\n        return\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert the OpenCV BGR frame to a PIL RGB image\n        # OpenCV uses BGR, PIL uses RGB\n        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n        # Apply the LUT using pillow-lut's filter method\n        filtered_image = pil_image.filter(lut)\n\n        # Convert the PIL RGB image back to an OpenCV BGR frame\n        output_frame = cv2.cvtColor(np.array(filtered_image), cv2.COLOR_RGB2BGR)\n\n        # Write the frame to the output video\n        out.write(output_frame)\n\n    # Release everything when job is finished\n    cap.release()\n    out.release()\n    print(f\"Video processing complete. Output saved to {output_video_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage:\ninput_vid = \"/kaggle/working/2026-02-05-Miliki_SEUMUR_HIDUP_5_Investasi_ETF_TERBAIK_Ini_untuk_KAYA_ABADI/rahasia_kaya_abadi_modal_1_juta_jadi_3_5_miliar_crop.mp4\"\noutput_vid = \"output_with_lut2.mp4\"\nlut = '/kaggle/input/video-podcast-samples/DarkAndSomber.cube'\n# apply_lut_to_video(input_vid, output_vid, lut)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ffmpeg-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ffmpeg\nimport os\n\ndef apply_lut_ffmpeg(input_video, output_video, lut_file):\n    \"\"\"\n    Applies a .cube LUT to a video using FFmpeg.\n    \"\"\"\n    if not os.path.exists(input_video):\n        print(f\"Error: Input video '{input_video}' not found.\")\n        return\n    if not os.path.exists(lut_file):\n        print(f\"Error: LUT file '{lut_file}' not found.\")\n        return\n\n    try:\n        # Create the input stream\n        stream = ffmpeg.input(input_video)\n        \n        # Apply the 'lut3d' filter\n        # We process the video stream, then attach the audio stream from the original\n        video = stream.video.filter('lut3d', file=lut_file)\n        audio = stream.audio\n        \n        # Output the result\n        out = ffmpeg.output(video, audio, output_video)\n        \n        # Run the command (overwrite_output allows replacing existing files)\n        out.run(overwrite_output=True)\n        print(f\"Success! Video saved to {output_video}\")\n        \n    except ffmpeg.Error as e:\n        print(\"An error occurred with FFmpeg:\")\n        print(e.stderr.decode('utf8'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Usage\n# apply_lut_ffmpeg('input.mp4', 'output_lut.mp4', 'my_look.cube')\napply_lut_ffmpeg(input_vid, output_vid, lut)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lut=\"/kaggle/input/video-podcast-samples/LushGreen.cube\"\noutput_lut = \"video_lut2.mp4\"\ncmd = [\n        \"ffmpeg\", '-y', \"-hwaccel\", \"cuda\",\n        \"-i\", input_vid,\n        \"-vf\", f\"lut3d={lut}\",\n        \"-c:v\", \"h264_nvenc\",\n        \"-preset\", \"fast\",\n        \"-cq\", \"20\",\n        \"-c:a\", \"copy\",\n        output_lut\n    ]\nsubprocess.run(cmd, check=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Video, display\n\n# Create and display the video object\ndisplay(Video(output_lut, width=300))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}